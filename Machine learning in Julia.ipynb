{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, PyCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Running `conda install -y pytorch` in root environment\n",
      "└ @ Conda /Users/khyatt/.julia/packages/Conda/3rPhK/src/Conda.jl:113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <module 'torch' from '/Users/khyatt/.julia/conda/3/lib/python3.7/site-packages/torch/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Conda\n",
    "Conda.add(\"pytorch\")\n",
    "pytorch = pyimport(\"torch\") # very simple to import a Python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a tokenized dictionary of the WikiData\n",
    "struct Corpus\n",
    "    word_dict::Dict{String, Int}\n",
    "    test_data\n",
    "    train_data \n",
    "    valid_data\n",
    "end\n",
    "\n",
    "function tokenize!(word_dict, path)\n",
    "    # add the unique words to the dictionary\n",
    "    for el in eachline(path)\n",
    "        words = vcat(split(el, \" \"), \"<eos>\")\n",
    "        for word in filter(word -> !haskey(word_dict, word), words)\n",
    "            word_dict[word] = length(word_dict)\n",
    "        end\n",
    "    end\n",
    "    # turn the words in the file into tokens\n",
    "    ids = Vector{Int}(undef, 0)\n",
    "    for el in eachline(path)\n",
    "        words = vcat(split(el, \" \"), \"<eos>\")\n",
    "        ids_  = zeros(Int, length(words))\n",
    "        for wi in 1:length(ids_)\n",
    "            ids_[wi] = get(word_dict, words[wi], 0)\n",
    "        end\n",
    "        append!(ids, ids_)\n",
    "    end\n",
    "    return word_dict, ids\n",
    "end\n",
    "\n",
    "function Corpus(data_path::String)\n",
    "    @assert isdir(data_path) \"Data filepath $data_path does not exist\"\n",
    "    word_dict = Dict{String, Int}()\n",
    "    word_dict, test_data  = tokenize!(word_dict, joinpath(data_path, \"test.txt\"))\n",
    "    word_dict, train_data = tokenize!(word_dict, joinpath(data_path, \"train.txt\"))\n",
    "    word_dict, valid_data = tokenize!(word_dict, joinpath(data_path, \"valid.txt\"))\n",
    "    Corpus(word_dict, test_data, train_data, valid_data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = Corpus(joinpath(pwd(), \"wikitext2\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchify the data\n",
    "batch_size    = 20\n",
    "eval_batch    = 10\n",
    "import Flux: chunk\n",
    "train_batches = chunk(wiki_data.train_data, div(length(wiki_data.train_data), batch_size));\n",
    "test_batches  = chunk(wiki_data.test_data, div(length(wiki_data.test_data), eval_batch));\n",
    "valid_batches = chunk(wiki_data.valid_data, div(length(wiki_data.valid_data), eval_batch));\n",
    "# data is ready, let's make a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden  = 200\n",
    "nlayers = 2\n",
    "Ntoken  = 200\n",
    "batch_size    = 20\n",
    "eval_batch    = 20\n",
    "seq_len       = 35\n",
    "throttler     = 20\n",
    "lr            = 1e-5\n",
    "epochs        = 2\n",
    "import Flux: chunk, onehot, onecold, onehotbatch, crossentropy, throttle\n",
    "using Statistics, Random\n",
    "using BSON: @save\n",
    "\n",
    "# use Flux's inbuilt RNN stuff\n",
    "function train(; kws...)\n",
    "    @info(\"Loading data....\")\n",
    "    wiki_data = Corpus(joinpath(pwd(), \"wikitext2\"))\n",
    "    train_batches = chunk(wiki_data.train_data, div(length(wiki_data.train_data), batch_size))\n",
    "    test_batches  = chunk(wiki_data.test_data,  div(length(wiki_data.test_data),  eval_batch))\n",
    "    valid_batches = chunk(wiki_data.valid_data, div(length(wiki_data.valid_data), eval_batch))\n",
    "\n",
    "    test_data = Vector{Tuple{Flux.OneHotMatrix, Vector{Int}}}(undef, length(test_batches))\n",
    "    for (ii, td) in enumerate(test_batches)\n",
    "        split = min(seq_len, length(td)-1)\n",
    "        test_data[ii] = (onehotbatch(td[1:split], 1:length(wiki_data.word_dict)), td[split+1:end])\n",
    "    end\n",
    "    train_data = Vector{Tuple{Flux.OneHotMatrix, Vector{Int}}}(undef, length(train_batches))\n",
    "    for (ii, td) in enumerate(train_batches)\n",
    "        split = min(seq_len, length(td)-1)\n",
    "        train_data[ii] = (onehotbatch(td[1:split], 1:length(wiki_data.word_dict)), td[split+1:end])\n",
    "    end\n",
    "    valid_data = Vector{Tuple{Flux.OneHotMatrix, Vector{Int}}}(undef, length(valid_batches))\n",
    "    for (ii, td) in enumerate(valid_batches)\n",
    "        split = min(seq_len, length(td)-1)\n",
    "        valid_data[ii] = (onehotbatch(td[1:split], 1:length(wiki_data.word_dict)), td[split+1:end])\n",
    "    end\n",
    "\n",
    "    @info(\"Constructing model....\")\n",
    "    encoding = Dense(length(wiki_data.word_dict), Ntoken)\n",
    "    decoding = Dense(Ntoken, length(wiki_data.word_dict))\n",
    "\n",
    "    model      = Chain(encoding, Dropout(0.5), LSTM(hidden, hidden), LSTM(Ntoken, hidden), Dropout(0.5), decoding, softmax)\n",
    "    loss(x, y) = crossentropy(model(x), onehotbatch(y, 1:length(wiki_data.word_dict)))\n",
    "    opt        = ADAM(lr)\n",
    "    evalcb     = () -> @show mean([loss(data[1], data[2]) for data in valid_data[1:10]])\n",
    "\n",
    "    @info(\"Training Model...\")\n",
    "    for epoch in 1:epochs\n",
    "        Flux.train!(loss, params(model), shuffle!(train_data[1:10]), opt, cb = throttle(evalcb, throttler))\n",
    "    end\n",
    "\n",
    "    @save \"wikimodel.bson\" model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading data....\n",
      "└ @ Main In[6]:16\n",
      "┌ Info: Constructing model....\n",
      "└ @ Main In[6]:38\n",
      "┌ Info: Training Model...\n",
      "└ @ Main In[6]:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 208.25589f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 208.2609f0\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.0-DEV",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
