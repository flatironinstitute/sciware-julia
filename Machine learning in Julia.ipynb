{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/projects/sciware-julia/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(@__DIR__); Pkg.instantiate()\n",
    "using Flux, PyCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a tokenized dictionary of the WikiData\n",
    "struct Corpus\n",
    "    word_dict::Dict{String, Int}\n",
    "    test_data\n",
    "    train_data \n",
    "    valid_data\n",
    "end\n",
    "\n",
    "function tokenize!(word_dict, path)\n",
    "    # add the unique words to the dictionary\n",
    "    for el in eachline(path)\n",
    "        words = vcat(split(el, \" \"), \"<eos>\")\n",
    "        for word in filter(word -> !haskey(word_dict, word), words)\n",
    "            word_dict[word] = length(word_dict)\n",
    "        end\n",
    "    end\n",
    "    # turn the words in the file into tokens\n",
    "    ids = Vector{Int}(undef, 0)\n",
    "    for el in eachline(path)\n",
    "        words = vcat(split(el, \" \"), \"<eos>\")\n",
    "        ids_  = zeros(Int, length(words))\n",
    "        for wi in 1:length(ids_)\n",
    "            ids_[wi] = get(word_dict, words[wi], 0)\n",
    "        end\n",
    "        append!(ids, ids_)\n",
    "    end\n",
    "    return word_dict, ids\n",
    "end\n",
    "\n",
    "function Corpus(data_path::String)\n",
    "    @assert isdir(data_path) \"Data filepath $data_path does not exist\"\n",
    "    word_dict = Dict{String, Int}()\n",
    "    word_dict, test_data  = tokenize!(word_dict, joinpath(data_path, \"test.txt\"))\n",
    "    word_dict, train_data = tokenize!(word_dict, joinpath(data_path, \"train.txt\"))\n",
    "    word_dict, valid_data = tokenize!(word_dict, joinpath(data_path, \"valid.txt\"))\n",
    "    Corpus(word_dict, test_data, train_data, valid_data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = Corpus(joinpath(pwd(), \"wikitext2\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchify the data\n",
    "batch_size    = 20\n",
    "eval_batch    = 10\n",
    "import Flux: chunk\n",
    "train_batches = chunk(wiki_data.train_data, div(length(wiki_data.train_data), batch_size));\n",
    "test_batches  = chunk(wiki_data.test_data, div(length(wiki_data.test_data), eval_batch));\n",
    "valid_batches = chunk(wiki_data.valid_data, div(length(wiki_data.valid_data), eval_batch));\n",
    "# data is ready, let's make a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden  = 200\n",
    "nlayers = 2\n",
    "Ntoken  = 200\n",
    "batch_size    = 20\n",
    "eval_batch    = 20\n",
    "seq_len       = 35\n",
    "throttler     = 20\n",
    "lr            = 1e-5\n",
    "epochs        = 2\n",
    "import Flux: chunk, onehot, onecold, onehotbatch, crossentropy, throttle\n",
    "using Statistics, Random\n",
    "using BSON: @save\n",
    "\n",
    "# use Flux's inbuilt RNN stuff\n",
    "function train(; kws...)\n",
    "    @info(\"Loading data....\")\n",
    "    wiki_data = Corpus(joinpath(pwd(), \"wikitext2\"))\n",
    "    train_batches = chunk(wiki_data.train_data, div(length(wiki_data.train_data), batch_size))\n",
    "    test_batches  = chunk(wiki_data.test_data,  div(length(wiki_data.test_data),  eval_batch))\n",
    "    valid_batches = chunk(wiki_data.valid_data, div(length(wiki_data.valid_data), eval_batch))\n",
    "\n",
    "    test_data = Vector{Tuple{Flux.OneHotMatrix, Vector{Int}}}(undef, length(test_batches))\n",
    "    for (ii, td) in enumerate(test_batches)\n",
    "        split = min(seq_len, length(td)-1)\n",
    "        test_data[ii] = (onehotbatch(td[1:split], 1:length(wiki_data.word_dict)), td[split+1:end])\n",
    "    end\n",
    "    train_data = Vector{Tuple{Flux.OneHotMatrix, Vector{Int}}}(undef, length(train_batches))\n",
    "    for (ii, td) in enumerate(train_batches)\n",
    "        split = min(seq_len, length(td)-1)\n",
    "        train_data[ii] = (onehotbatch(td[1:split], 1:length(wiki_data.word_dict)), td[split+1:end])\n",
    "    end\n",
    "    valid_data = Vector{Tuple{Flux.OneHotMatrix, Vector{Int}}}(undef, length(valid_batches))\n",
    "    for (ii, td) in enumerate(valid_batches)\n",
    "        split = min(seq_len, length(td)-1)\n",
    "        valid_data[ii] = (onehotbatch(td[1:split], 1:length(wiki_data.word_dict)), td[split+1:end])\n",
    "    end\n",
    "\n",
    "    @info(\"Constructing model....\")\n",
    "    encoding = Dense(length(wiki_data.word_dict), Ntoken)\n",
    "    decoding = Dense(Ntoken, length(wiki_data.word_dict))\n",
    "\n",
    "    model      = Chain(encoding, Dropout(0.5), LSTM(hidden, hidden), LSTM(Ntoken, hidden), Dropout(0.5), decoding, softmax)\n",
    "    loss(x, y) = crossentropy(model(x), onehotbatch(y, 1:length(wiki_data.word_dict)))\n",
    "    opt        = ADAM(lr)\n",
    "    evalcb     = () -> @show mean([loss(data[1], data[2]) for data in valid_data[1:10]])\n",
    "\n",
    "    @info(\"Training Model...\")\n",
    "    for epoch in 1:epochs\n",
    "        Flux.train!(loss, params(model), shuffle!(train_data[1:1_000]), opt, cb = throttle(evalcb, throttler))\n",
    "    end\n",
    "\n",
    "    @save \"wikimodel.bson\" model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading data....\n",
      "└ @ Main In[7]:16\n",
      "┌ Info: Constructing model....\n",
      "└ @ Main In[7]:38\n",
      "┌ Info: Training Model...\n",
      "└ @ Main In[7]:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 208.2052f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 208.11629f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 208.0258f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 207.92195f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 207.81845f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 207.70615f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 207.55449f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 207.36362f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 207.09106f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 206.68428f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 206.1312f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 205.1016f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 203.41951f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 200.53401f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 196.5569f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 191.89404f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 188.13028f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 184.94843f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 182.36568f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 180.04688f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 179.05392f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 176.96103f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 175.18771f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 173.5653f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 172.19376f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 170.92827f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 169.90796f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 168.94592f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 168.01822f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 167.3046f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 166.68051f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 166.08325f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 165.64735f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 165.16911f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 164.71298f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 164.35689f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 164.08578f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 163.85953f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 163.7474f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 163.6432f0\n",
      "mean([loss(data[1], data[2]) for data = valid_data[1:10]]) = 163.53642f0\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
